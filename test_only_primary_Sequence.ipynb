{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "necuqYODTh8O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import r2_score\n",
        "import random\n",
        "import matplotlib as mpl\n",
        "import os\n",
        "import gc\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import date\n",
        "mpl.rcParams['figure.dpi'] = 180"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyper-parameters\n",
        "# batch_size = 11\n",
        "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM Model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, num_layers, seq_len,num_classes=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size1 = hidden_size1\n",
        "        self.hidden_size2 = hidden_size2\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.bnn1 = nn.Linear(input_size, 32)\n",
        "        self.bnn2 = nn.Linear(32,64)\n",
        "        self.bnn3 = nn.Linear(64,64)\n",
        "        self.bnn4 = nn.Linear(64,64)\n",
        "\n",
        "        self.lstm1 = nn.LSTM(64, hidden_size1, num_layers, batch_first=True, bidirectional=True, dropout=0.5)\n",
        "        self.bn_lstm1 = nn.BatchNorm1d(2*hidden_size1,device=device)  \n",
        "        # self.lstm2 = nn.LSTM(2*hidden_size1, hidden_size2, num_layers, batch_first=True, bidirectional=True, dropout=0.5)\n",
        "        # self.bn_lstm2 = nn.BatchNorm1d(2*hidden_size2,device=device)\n",
        "        self.nn1 = nn.Linear(2*hidden_size1, 2*hidden_size1)\n",
        "        self.nn2 = nn.Linear(2*hidden_size1, 512)\n",
        "        self.nn3 = nn.Linear(512, 512)\n",
        "        self.nn4 = nn.Linear(512, 256)\n",
        "        self.nn5 = nn.Linear(256, 256)\n",
        "        self.nn6 = nn.Linear(256, 128)\n",
        "        self.nn7 = nn.Linear(128, 32)\n",
        "        self.nn8 = nn.Linear(32, 1)\n",
        "\n",
        "    \n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "        # self.batch = nn.BatchNorm1d()\n",
        "        self.drop = nn.Dropout(p=0.5)\n",
        "\n",
        "\n",
        "        \n",
        "    def forward(self, x, array_lengths):\n",
        "        # Set initial hidden states (and cell states for LSTM)\n",
        "        # print(x.size(0))\n",
        "        inital_seq_len = x.size(1)\n",
        "        x = Variable(x.float()).to(device)\n",
        "\n",
        "        x = torch.reshape(x, (x.size(0)*x.size(1), x.size(2)))\n",
        "\n",
        "        ## before nn\n",
        "        out = self.bnn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.bnn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.bnn3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.bnn4(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        ## reshaping again\n",
        "        out = torch.reshape(out, (-1, inital_seq_len, out.size(1)))\n",
        "        # print(out.size())\n",
        "        # print(aaaaa)\n",
        "\n",
        "        # out = torch.permute(out, (0,2,1))\n",
        "        \n",
        "        pack = nn.utils.rnn.pack_padded_sequence(out, array_lengths, batch_first=True, enforce_sorted=False)\n",
        "        h0 = Variable(torch.zeros(2*self.num_layers, x.size(0), self.hidden_size1).to(device))\n",
        "        c0 = Variable(torch.zeros(2*self.num_layers, x.size(0), self.hidden_size1).to(device))\n",
        "        h1 = Variable(torch.zeros(2*self.num_layers, self.hidden_size1, self.hidden_size2).to(device))\n",
        "        c1 = Variable(torch.zeros(2*self.num_layers, self.hidden_size1, self.hidden_size2).to(device))\n",
        "        \n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.lstm1(pack, (h0,c0))\n",
        "        del(h0)\n",
        "        del(c0)\n",
        "        # out, _ = self.lstm2(out, (h1,c1))\n",
        "        gc.collect()\n",
        "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        this_batch_len = unpacked.size(1)\n",
        "        out = unpacked\n",
        "        # print('before', out.size())\n",
        "        out = torch.reshape(out, (out.size(0)*out.size(1), out.size(2)))\n",
        "\n",
        "        ##nn\n",
        "        out = self.nn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.nn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.nn3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.nn4(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.nn5(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.nn6(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.nn7(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.nn8(out)\n",
        "        \n",
        "        ## reshaping\n",
        "        out = torch.reshape(out, (-1, this_batch_len, 1))\n",
        "        # print(out.size()) \n",
        "        # print(aaaaa)   \n",
        "        \n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoch_check =177\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device is',device)\n",
        "\n",
        "model_test = torch.load('/home/apa2237/Protein_BetaFactor/Model_35_primary/epoch_'+ str(epoch_check) + '.pth', map_location='cuda')\n",
        "model_test.eval().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test = np.load('x_test_35_pri.npy', allow_pickle=True)\n",
        "# dataset preparation\n",
        "class BetaDataset(Dataset) :\n",
        "    def __init__(self,x,y, n_samples) :\n",
        "        # data loading\n",
        "        self.x = x\n",
        "        self.y = y \n",
        "        self.n_samples = n_samples\n",
        "        \n",
        "        \n",
        "    def __getitem__(self,index) :\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self) :    \n",
        "        return self.n_samples      \n",
        "\n",
        "test_dataset = BetaDataset(x_test,x_test,np.shape(x_test)[0])\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                          batch_size=128,\n",
        "                          shuffle=False,\n",
        "                          num_workers=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import nan\n",
        "\n",
        "all_x = np.load('/home/apa2237/Protein_BetaFactor/dataloader_betanormalized/x_61046' + '.npy', allow_pickle=True)\n",
        "all_y = np.load('/home/apa2237/Protein_BetaFactor/dataloader_betanormalized/y_61046' + '.npy', allow_pickle=True)\n",
        "\n",
        "# test_loader = test_loader[15:20,:]\n",
        "p_beta = torch.zeros((1)).to(device)\n",
        "a_beta = torch.zeros((1)).to(device)\n",
        "collection_test = 0\n",
        "avg_pearson_count = 0\n",
        "avg_pearson = 0\n",
        "count_all = 0\n",
        "with torch.no_grad():\n",
        "  for i, (parameters, no_req) in enumerate(test_loader):\n",
        "    # parameters = parameters.to(device)\n",
        "    input_x = torch.from_numpy(all_x[parameters,:,:]).to(device)\n",
        "    output_y = torch.from_numpy(all_y[parameters,:,:]).to(device)\n",
        "    input_x = torch.reshape(input_x, (input_x.size(0), input_x.size(2), input_x.size(3)))\n",
        "    output_y = torch.reshape(output_y, (output_y.size(0), output_y.size(2), output_y.size(3)))\n",
        "    \n",
        "    # forward pass  \n",
        "    array_lengths = input_x[:,0,28]\n",
        "    # print('step 1', type(array_lengths))\n",
        "    array_lengths = array_lengths.int()\n",
        "    # print('step 2', type(array_lengths))\n",
        "    array_lengths = array_lengths.tolist()\n",
        "    # print('step 3', type(array_lengths))\n",
        "    # print(array_lengths.shape)\n",
        "    # print(aaaa)\n",
        "    outputs = model_test(input_x[:,:,0:21], array_lengths)\n",
        "    outputs = torch.reshape(outputs, (-1,int(max(array_lengths))) )\n",
        "    output_y = torch.reshape(output_y[:,0:int(max(array_lengths)), 0], (-1,int(max(array_lengths))))\n",
        "\n",
        "    outputs = outputs.T\n",
        "    output_y = output_y.T\n",
        "\n",
        "    for j in range(input_x.size()[0]):    \n",
        "      prot_len = int(input_x[j,0,28].item())\n",
        "      # print(prot_len)\n",
        "      if prot_len != 0:\n",
        "        a_beta = torch.cat((torch.flatten(output_y[0:prot_len,j].float()), a_beta.float()),0)\n",
        "        p_beta = torch.cat((torch.flatten(outputs[0:prot_len,j].float()), p_beta.float()),0)\n",
        "        \n",
        "        if math.isnan(np.corrcoef(outputs[0:prot_len,j].cpu(), output_y[0:prot_len,j].cpu())[0,1]):\n",
        "          print('nan')\n",
        "        else:\n",
        "          # print(np.corrcoef(outputs[0:prot_len,j].cpu(), output_y[0:prot_len,j].cpu())[0,1])\n",
        "          avg_pearson =  (avg_pearson*avg_pearson_count + np.corrcoef(outputs[0:prot_len,j].cpu(), output_y[0:prot_len,j].cpu())[0,1])/(avg_pearson_count+1)\n",
        "          avg_pearson_count += 1\n",
        "          count_all = count_all + prot_len\n",
        "\n",
        "    # print(torch.flatten(outputs.float()).size())\n",
        "\n",
        "    # a_beta = torch.cat((torch.flatten(output_y.float()), a_beta.float()),0)\n",
        "    # p_beta = torch.cat((torch.flatten(outputs.float()), p_beta.float()),0)\n",
        "\n",
        "    \n",
        "    del(input_x)\n",
        "    del(output_y)\n",
        "    gc.collect()\n",
        "\n",
        "a_beta = a_beta[1:,]\n",
        "p_beta = p_beta[1:,]\n",
        "\n",
        "print('Total data collected', a_beta.size(0))\n",
        "print('Total data predicted', p_beta.size(0))\n",
        "# print('Total point should be',count_all)\n",
        "print('Avgerage Pearson coefficient is', avg_pearson)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# combined = np.zeros((1,a_ebya))\n",
        "with torch.no_grad():\n",
        "    plt.figure(1)\n",
        "    # plt.rcParams.update({'font.size': 16})\n",
        "    plt.scatter(a_beta.cpu(), p_beta.cpu(), s=0.5)\n",
        "    plt.plot(a_beta.cpu(), a_beta.cpu(),'k')\n",
        "    # plt.title(f'R2 score:{r2_score(a_beta.cpu(), p_beta.cpu())}')\n",
        "    plt.title(f'Pearson Correlation Coefficient:{round(avg_pearson,2)}, R^2 is {round(r2_score(a_beta.cpu(), p_beta.cpu()),2)}')\n",
        "    # plt.plot(p_beta[50000:100000].cpu())\n",
        "    plt.xlabel('Actual Normalized Beta factor')\n",
        "    plt.ylabel('Predicted Normalized Beta factor')\n",
        "    # plt.xlim([0,80])\n",
        "    # plt.ylim([0,80])\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Unzipping_and_counting_files.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15 (default, Nov  4 2022, 20:59:55) \n[GCC 11.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "98604b3b45ec6a06ff44cc545e23221fcb3349f85795d5ab7ba8ebf01f1c77b6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
